from collections import defaultdict
from datetime import datetime
import queue
import requests
import variables
import urllib.request
import json
import time
import csv
import paramiko
import sys
import threading
from multiprocessing import Event, Process
from multiprocessing.pool import ThreadPool
import subprocess
import os
import signal
from power import pw, em
from variables import *
import k8s_API
from time import sleep
import re
import psutil


def get_bytes():
    return psutil.net_io_counters().bytes_sent + psutil.net_io_counters().bytes_recv

# def get_mbits():
#     return (psutil.net_io_counters().bytes_sent + psutil.net_io_counters().bytes_recv)/1024./1024.*8


def thread_remote(cmd: str):
    thread = threading.Thread(target=remote_worker_call, args=(cmd, )).start()
    return thread


def remote_worker_call(command: str, event=None):
    print("Trying to connect to remote host {}, IP: {}".format(
        JETSON_USERNAME, JETSON_IP))
    try:
        client = paramiko.SSHClient()
        client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        client.connect(JETSON_IP, username=JETSON_USERNAME,
                       password=JETSON_PASSWORD)
    except paramiko.AuthenticationException:
        print("Authentication failed when connecting to %s" % JETSON_IP)
        sys.exit(1)
    except:
        print("Could not SSH to %s, waiting for it to start" % JETSON_IP)
    print(command)
    stdin, stdout, stderr = client.exec_command(command, get_pty=True)
    stdin.write(JETSON_PASSWORD + '\n')
    stdin.flush()
    for line in stdout:
        print(line.strip('\n'))
    client.close()
    if event is not None:
        event.set()


def get_data_from_api(query: str):
    url_data = PROMETHEUS_DOMAIN + query
    try:
        contents = urllib.request.urlopen(url_data).read().decode('utf-8')
        values = json.loads(contents)["data"]["result"][0]['value']
    except:
        values = -1
    return values

# def get_power():
#     # print(pw.get_power()/1000.0)
#     return pw.get_power()/1000.0

# def is_pod_terminating(): # check pod is terminating? consider replacing this function with Py-python
#     status = str(subprocess.run(['deployments/get_pod_status.sh', '-l'], stdout=subprocess.PIPE).stdout.decode('utf-8')).strip()
#     return status == TERMINATING_STATUS


def get_curl_values_and_update_job(cmd: str, host: str, image: str, target_pods: int, job: str, quality: str, repetition: int):
    # Run the command and capture its output
    status, results = exec_pod(cmd, target_pods, "curl_time")
    # print("Size of results: {}".format(len(results)))
    for i, output in enumerate(results, start=1):
        # output_t = output.decode("utf-8")
        # print("Type of output: {}".format(type(output)))
        # print(output)
        if b"OK" in output or b"Active" in output:
            print("'OK' request {}".format(i))
        else:
            print("Error in request {}".format(i))
        output = output.replace(b",", b".")

        time_namelookup = float(output.split(
            b"time_namelookup:  ")[1].split(b" ")[0])
        time_connect = float(output.split(b"time_connect:  ")[1].split(b" ")[0])
        time_appconnect = float(output.split(
            b"time_appconnect:  ")[1].split(b" ")[0])
        time_pretransfer = float(output.split(
            b"time_pretransfer:  ")[1].split(b" ")[0])
        time_redirect = float(output.split(b"time_redirect:  ")[1].split(b" ")[0])
        time_starttransfer = float(output.split(
            b"time_starttransfer:  ")[1].split(b" ")[0])
        time_total = float(output.split(b"time_total:  ")[1].split(b" ")[0])

        # Store the values in a dictionary
        # time_dict = {"time_namelookup": time_namelookup, "time_connect": time_connect, "time_appconnect": time_appconnect, "time_pretransfer": time_pretransfer,
        #           "time_redirect": time_redirect, "time_starttransfer": time_starttransfer, "time_total": time_total}

        # Write value to data file
        try:
            writer = csv.writer(open(DATA_CURL_FILE_DIRECTORY.format(
                str(host), str(image), str(target_pods), str(repetition), generate_file_time), 'a'))
            writer.writerow([time_namelookup, time_connect, time_appconnect,
                            time_pretransfer, time_redirect, time_starttransfer, time_total, job, quality])
        except Exception as ex:
            print(ex)


def get_prometheus_values_and_update_job(host: str, image: str, target_pods: int, state: str, repetition: int):
    if host == 'jetson':
        values_power = pw.get_power()/1000.0
    else:
        voltage, current, energy, real_power, apparent_power, reactive_power, power_factor, frequency = em.get_energy_data()
        # real_power = 100000
        values_power = real_power/100.0
    values_nw = get_bytes()
    values_per_cpu_in_use = get_data_from_api(
        VALUES_CPU_QUERY.format(JETSON_IP))
    values_per_gpu_in_use = get_data_from_api(
        VALUES_GPU_QUERY.format(JETSON_IP))
    # values_network_receive = get_data_from_api(VALUES_NETWORK_RECEIVE_QUERY)
    values_memory = get_data_from_api(
        VALUES_MEMORY_QUERY.format(JETSON_IP, JETSON_IP, JETSON_IP))
    # print(values_memory)
    values_running_pods = k8s_API.get_number_pod()
    # print(values_running_pods)

    # write values to file
    try:
        writer = csv.writer(open(DATA_PROMETHEUS_FILE_DIRECTORY.format(
            str(host), str(image), str(target_pods), str(repetition), generate_file_time), 'a'))
        writer.writerow([values_memory[0], datetime.utcfromtimestamp(values_memory[0]).strftime('%Y-%m-%d %H:%M:%S.%f')[:-3], values_running_pods,
                         values_power, values_per_cpu_in_use[1], values_per_gpu_in_use[1], values_memory[1], values_nw, state])
    except Exception as ex:
        print(ex)
    # if TEST_MODE: print("Current pods: %s, target: %d" % (curr_pods, (int(target_pods)+POD_EXSISTED)))


# def update_job_status(state:str, values_running_pods, target_pods:int):
#     #+2 on target pods for the default pods
#     curr_running_pods = int(values_running_pods[1])
#     # print(state, values_running_pods, target_pods, POD_EXISTED)
#     if WARM_DISK_2_WARM_CPU_PROCESS == state or "cold_start:curl" == state:
#         if curr_running_pods == POD_EXISTED + target_pods:
#             jobs_status[WARM_DISK_TO_WARM_CPU_PROCESS] = False
#     elif WARM_CPU_STATE == state:
#         if curr_running_pods == POD_EXISTED: # it means pods have been deleted
#             jobs_status[WARM_CPU_STATE] = False
#     elif DELETE_JOB == state:
#         if curr_running_pods == POD_EXISTED:
#             jobs_status[DELETE_PROCESSING] = False

# NOTE: Tung will handle this function
# def create_request(url:str): # Here change to kubectl exec command by k8s python
#     #rs_response = requests.get(url)
#     rs_response = kubectl exec -it ubuntu -- "url"
#     print(rs_response.content)

def bash_cmd(cmd: str):
    result = subprocess.run([cmd], stderr=subprocess.PIPE, text=True)
    print("Bash output: {}".format(result.stderr))

# def create_request_thread(target_pods: int, request_type: str):
#     for i in range(target_pods):
#         print("Start thread :", i + 1)
#         if request_type == "start":
#             threading.Thread(target=create_request, args=("http://detection"+str(i+1)+".default.svc.cluster.local/{}/api/stream/{}/{}".format(STREAMING_IP, DETECTION_TIME))).start()
#         else if request_type == "stop":
#             threading.Thread(target=create_request, args=("http://detection"+str(i+1)+".default.svc.cluster.local/api/terminate")).start()
#         else:
#             break


def timestamps_to_file(host: str, image: str, timestamps: dict, target_pods: int, repetition: int):
    # print(timestamps)
    with open(DATA_TIMESTAMP_FILE_DIRECTORY.format(
            str(host), str(image), str(target_pods), str(repetition), generate_file_time), 'w') as f:
        # for key, value in terminate_state.items():
        #     timestamps[key+"_start"]=min(value)
        #     timestamps[key+"_end"]=max(value)
        for key in timestamps.keys():
            if "_start" in key:
                job_key = re.search('(.*)_start', key).group(1)
            if "_end" in key:
                job_key = re.search('(.*)_end', key).group(1)
            f.write("%s,%s,%s\n" % (key, timestamps[key], job_key))

# NOTE: the following function auto terminate pods


def auto_delete(target_pod, event):
    token = True
    while not event.is_set():
        if k8s_API.is_pod_terminated() and not k8s_API.is_all_con_not_ready() and token:
            print("Detect terminating pod, it'll be deleted shortly")
            if exec_pod(CURL_TERM, target_pod, "auto_delete"):
                token = False
            else:
                print("Try to terminate pod, but IP returns None, will try again!")
                token = True
        elif not k8s_API.is_pod_terminated():
            # print("Status is: {}, token is: {}".format(k8s_API.is_pod_terminated(), token))
            token = True
        else:
            # print("Status is: {}, token is: {}".format(k8s_API.is_pod_terminated(), token))
            sleep(1)
        # print("In terminating while loop")
    print("Overwatch for termination finished!")


# def exec_pod(cmd: str, type: str = "normal"):
#     results = []
#     threads = []
#     IPs = []
#     result_queue = queue.Queue()
#     output_lock = threading.Lock()
#     status = True

#     if type == "auto_delete":
#         list_pod = k8s_API.get_list_term_pod(NAMESPACE)
#     else:
#         list_pod = k8s_API.list_namespaced_pod_status(NAMESPACE)
    
#     for i in list_pod:
#         IP = i.pod_ip
#         if (IP is None):
#             return False, results
#         else:
#             IPs.append(IP)
#     for ip in IPs:
#         t = threading.Thread(target=connect_pod_exec, args=(cmd.format(ip), result_queue, output_lock, ))
#         threads.append(t)
#     for t in threads:
#         t.start()
#     for t in threads:
#         t.join()
    
#     while not result_queue.empty():
#         result = result_queue.get()
#         results.append(result)
#     status = True
#     return status, results

def exec_pod(cmd: str, target_pod: int, type: str = "normal"):
    results = []
    threads = []
    IPs = []
    result_queue = queue.Queue()
    output_lock = threading.Lock()
    status = True
    if type == "auto_delete":
        list_pod = []
        while len(list_pod) < target_pod: # when multiple pods are deployed, sometimes the code can't query the number of term pod correctly
            list_pod = k8s_API.get_list_term_pod(NAMESPACE)
            print("Query of list_term_pod is {}, while target_pod is {}".format(len(list_pod), target_pod))
        for i in list_pod:
            t = threading.Thread(target=connect_pod_exec, args=(cmd.format(i.pod_ip), result_queue, output_lock, ))
            threads.append(t)
    else:
        for i in range(1, target_pod + 1, 1):
            t = threading.Thread(target=connect_pod_exec, args=(cmd.format(i), result_queue, output_lock, ))
            threads.append(t)
    for t in threads:
        t.start()
    for t in threads:
        t.join()
    while not result_queue.empty():
        result = result_queue.get()
        results.append(result)
    status = True
    return status, results

def connect_pod_exec(target_command: str, result_queue, lock, target_name: str = "ubuntu"):
    print(target_command)
    command = "kubectl exec -it {} -- {} ".format(target_name, target_command)
    trial = 0
    while trial < 20:
        try:
            output = subprocess.check_output(['/bin/bash', '-c', command]) # or check_output
            with lock:
                result_queue.put(output)
        except subprocess.CalledProcessError as e:
            output = str(e.output)
            # with output_lock:
            #     print("Subprocess output is: {}".format(output))
            if "52" in output:
                # with output_lock:
                #     print("Terminated successfully")
                return 
            else:
                # with output_lock:
                #     print("Terminated unsuccessfully, trial: {}".format(trial))
                sleep(1)
                trial = trial + 1
                continue
        # with output_lock:
        #     print("Seem like a good request, but we never know :)")
        return output
    # with output_lock:
    #     print("The system has sent {} times curl cmd, but none returns successfully.".format(trial))

def config_deploy(cmd: str):
    Process(target=k8s_API.config_deploy, args=(cmd, )).start()
    # threading.Thread(target= k8s_API.config_deploy, args=(cmd, )).start()


if __name__ == "__main__":
    # print(pw.get_power()/1000.0)
    # remote_worker_call("sudo ls -a")
    # sleep(100)
    # thread_event = threading.Event()
    # remote_worker_call(DELETE_IMAGE_CMD, thread_event)
    # get_prometheus_values_and_update_job('mec', 'image', 1, 1, '1')
    # sudo ctr images remove docker.io/kienkauko/nettools:latest@sha256:573c90a86216c26c02b27ce4105ea7cbf09016659fd30e8f8f61f67fab324620

    output = subprocess.check_output(['/bin/bash', '-c', 'curl -w \"@curl-time.txt\" google.com'])
        # print(output)

        # Extract the values you're interested in from the output
        # print(output.split(b"time_pretransfer:  ")[1].split(b" ")[0])

    time_namelookup = float(output.split(
        b"time_namelookup:  ")[1].split(b" ")[0])
    time_connect = float(output.split(b"time_connect:  ")[1].split(b" ")[0])
    time_appconnect = float(output.split(
        b"time_appconnect:  ")[1].split(b" ")[0])
    time_pretransfer = float(output.split(
        b"time_pretransfer:  ")[1].split(b" ")[0])
    time_redirect = float(output.split(b"time_redirect:  ")[1].split(b" ")[0])
    time_starttransfer = float(output.split(
        b"time_starttransfer:  ")[1].split(b" ")[0])
    time_total = float(output.split(b"time_total:  ")[1].split(b" ")[0])
    # Store the values in a dictionary
    time_dict = {"time_namelookup": time_namelookup, "time_connect": time_connect, "time_appconnect": time_appconnect, "time_pretransfer": time_pretransfer,
                "time_redirect": time_redirect, "time_starttransfer": time_starttransfer, "time_total": time_total}
    print(time_dict)
    # Write value to data file
    # try:
    #     writer = csv.writer(open(DATA_CURL_FILE_DIRECTORY.format(
    #         str(host), str(image), str(target_pods), str(repetition), generate_file_time), 'a'))
    #     writer.writerow([time_namelookup, time_connect, time_appconnect,
    #                     time_pretransfer, time_redirect, time_starttransfer, time_total, job])
    # except Exception as ex:
    #     print(ex)
